{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d764b01",
   "metadata": {},
   "source": [
    "# This notebook does the following: \n",
    "    \n",
    "1. Scrape multiple pages on imdb.com to get a list of notable movies\n",
    "2. Google search these movies to find its rotten tomatoes page\n",
    "3. Scrape the movie homepage on rotten tomatoes to get its known consensus summary\n",
    "    - this text will be used to create as target labels for a fine tuned summarization model \n",
    "4. Scrape the movie user/critic reviews\n",
    "    - this text will be used to create as training input for a fine tuned summarization model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e9f894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import google\n",
    "from tqdm import tqdm\n",
    "session = requests.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a9869b",
   "metadata": {},
   "source": [
    "# setting up for scrape of imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7d2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = '''\n",
    "Action\n",
    "Adventure\n",
    "Animation\n",
    "Biography\n",
    "Comedy\n",
    "Crime\n",
    "Documentary\n",
    "Drama\n",
    "Family\n",
    "Fantasy\n",
    "Film Noir\n",
    "History\n",
    "Horror\n",
    "Music\n",
    "Musical\n",
    "Mystery\n",
    "Romance\n",
    "Sci-Fi\n",
    "Sport\n",
    "Superhero\n",
    "Thriller\n",
    "War\n",
    "Western\n",
    "'''    \n",
    "\n",
    "categories = categories.lower().strip().split('\\n')\n",
    "print(len(categories))\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561a4a5f",
   "metadata": {},
   "source": [
    "# 1. Scraping IMDB \n",
    "\n",
    "## Scraping top 250 movies from all movie genres on imdb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de93ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_dict = dict()\n",
    "for genre in tqdm(categories[:]):\n",
    "    movie_dict[genre] = list()\n",
    "    for count in range(1, 251, 50):\n",
    "        try: \n",
    "            base_url = f'https://www.imdb.com/search/title/?title_type=feature&num_votes=25000,&genres={genre}&view=simple&sort=user_rating,desc&start={count}&ref_=adv_nxt'\n",
    "            response = requests.get(base_url)\n",
    "            page_contents = response.text\n",
    "\n",
    "            soup = BeautifulSoup(page_contents, 'html.parser') \n",
    "            review_text = soup.findAll(class_=\"lister-list\")\n",
    "            url_target_content = review_text[0].findAll(class_=\"col-title\")\n",
    "\n",
    "            for i in url_target_content:\n",
    "                title = i.text.strip().split('\\n')[2].strip()\n",
    "                movie_dict[genre].append(title)\n",
    "        except: \n",
    "            print(f'failed at {genre} at {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fbeb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_movies_list = []\n",
    "for g in movie_dict.keys():\n",
    "    all_movies_list.extend(movie_dict[g])\n",
    "    \n",
    "len(all_movies_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc77f536",
   "metadata": {},
   "source": [
    "# 2) Google each movie to get exact movie title url for rotten tomatoes \n",
    "# 3) then scrape movie homepage on rotten tomatoes\n",
    "\n",
    "Note: rotten tomatoes doesn't want there search used in scraping so using google search is the work around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67bba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c111558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_movie(query):\n",
    "    '''\n",
    "    returns top result for google search query\n",
    "    '''\n",
    "    return [i for i in search(query, num=1, stop=1, pause=2)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rt_consensus(url = str) -> tuple:\n",
    "    '''\n",
    "    Extracts critic and user summaries when they are on the homepage\n",
    "    of the rt movie title \n",
    "    '''\n",
    "    base_url = url\n",
    "    response = requests.get(base_url)\n",
    "    page_contents = response.text\n",
    "    soup = BeautifulSoup(page_contents, 'html.parser') \n",
    "    review_text = soup.findAll('p' , class_=\"what-to-know__section-body\")\n",
    "    \n",
    "    try: \n",
    "        critic = review_text[0].findAll('span')[0].text\n",
    "    except:\n",
    "         critic = None\n",
    "    try: \n",
    "        user = review_text[1].findAll('span')[0].text\n",
    "    except:\n",
    "         user = None\n",
    "    \n",
    "    return critic, user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb5c5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Note: tasks 2. and 3. are in same loop \n",
    "info = []\n",
    "count = 0\n",
    "for movie in tqdm(all_movies_list[:2]):\n",
    "    try: \n",
    "        query = f'{movie} rotten tomatoes'\n",
    "        url =google_movie(query) #task number 2\n",
    "        critic, user = get_rt_consensus(url = url)#task number 3\n",
    "        info.append((url, movie, critic, user))\n",
    "\n",
    "    except:\n",
    "        failed.append(movie)\n",
    "    count += 1 \n",
    "    if count  % 50 == 0:\n",
    "        print(f'count is at {count} so writing out data')\n",
    "        df = pd.DataFrame(info, columns = ['url', 'movie', 'critic_summary', 'user_summary'])\n",
    "        df.to_csv('movie_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef73af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving out final dataframe\n",
    "df.to_csv('movie_data_final.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a40dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(info, columns = ['url', 'movie', 'critic_summary', 'user_summary'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ae00b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df), len(all_movies_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a962e15",
   "metadata": {},
   "source": [
    "# 3) Scrape rotten romatoes for critic/user reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e6b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapeRtForDB(): \n",
    "    '''\n",
    "    Instance scrapes rotten tomatoes reviews\n",
    "    ... \n",
    "    Attributes: \n",
    "        movie_title: str \n",
    "            Title of the movie, \n",
    "        reviewer: str \n",
    "            Select reviewer pool from list: ['critic', 'user']. Default is user. \n",
    "        scrape_limit: int \n",
    "            Number of pages to stop scraping. Default is 10. \n",
    "        write_data : boolean\n",
    "            Writes data out to pickle object. Default is False. \n",
    "    Methods: \n",
    "        scrape_reviews\n",
    "            Scrapes rotten tomatoe page\n",
    "        filter_df\n",
    "            Filters scraped df \n",
    "        write_df\n",
    "            Writes out data\n",
    "        run_for_reviews\n",
    "            Runs all methods above\n",
    "    '''\n",
    "    def __init__(\n",
    "        self, \n",
    "        movie_title = '',\n",
    "        reviewer = 'user',\n",
    "        scraping_limit = 10, \n",
    "        write_data = False\n",
    "    ):\n",
    "    \n",
    "        self.movie_title = movie_title\n",
    "        self.reviewer = reviewer\n",
    "        self.scraping_limit = scraping_limit\n",
    "        self.write_data = write_data\n",
    "        self.review_df = None\n",
    "        self.user_cols = ['rating', 'review', 'displayName', 'isVerified', 'isSuperReviewer']\n",
    "        self.critic_cols = ['creationDate', 'isFresh', 'isRotten', 'isTop', 'reviewUrl',\n",
    "                           'quote','scoreSentiment', 'critic.name','publication.name']\n",
    "        print('\\n --------------------- \\n ')\n",
    "        print('Scrapped Initiated')\n",
    "        \n",
    "    def scrape_reviews(self):\n",
    "        '''\n",
    "        Scrapes rotten tomatoes for reviews and related info, and updates self.review_df with data \n",
    "        '''\n",
    "        url = f'https://www.rottentomatoes.com/m/{self.movie_title}/reviews'\n",
    "        r = requests.get(url)\n",
    "        movie_id = re.findall(r'(?<=movieId\":\")(.*)(?=\",\"type)',r.text)[0]\n",
    "\n",
    "        if self.reviewer == 'critic':\n",
    "            api_url = f\"https://www.rottentomatoes.com/napi/movie/{movie_id}/criticsReviews/all\"\n",
    "        if self.reviewer == 'user':\n",
    "            api_url = f\"https://www.rottentomatoes.com/napi/movie/{movie_id}/reviews/user\"\n",
    "        payload = {\n",
    "            'direction': 'next',\n",
    "            'endCursor': '',\n",
    "            'startCursor': '',\n",
    "        }\n",
    "        pages_scraped = 0\n",
    "        review_data = []\n",
    "        while True:\n",
    "            r = session.get(api_url, \n",
    "                      params=payload)\n",
    "            data = r.json()\n",
    "\n",
    "            if not data['pageInfo']['hasNextPage']:\n",
    "                print('Scaping completed')\n",
    "                break\n",
    "            elif pages_scraped == self.scraping_limit:\n",
    "                print('Scraping limit reached')\n",
    "                break\n",
    "\n",
    "            payload['endCursor'] = data['pageInfo']['endCursor']\n",
    "            payload['startCursor'] = data['pageInfo']['startCursor'] if data['pageInfo'].get('startCursor') else ''\n",
    "            review_data.extend(data['reviews'])\n",
    "            time.sleep(.1)\n",
    "            pages_scraped += 1\n",
    "            if pages_scraped % 50 == 0: \n",
    "                print(f'Pages scraped: {pages_scraped}')\n",
    "#         print(review_data)\n",
    "        self.review_df =  pd.json_normalize(review_data)\n",
    "        return \n",
    "        \n",
    "    def filter_df(self):\n",
    "        '''\n",
    "        takes in self.review_df and updates it based on filtering conditionals\n",
    "        \n",
    "        NOTE: if there are not enough verified user reviews (>5), no filtering carried out. \n",
    "        '''\n",
    "    \n",
    "        if self.reviewer == 'user':\n",
    "            self.review_df = self.review_df[self.user_cols].copy()\n",
    "        elif self.reviewer == 'critic':\n",
    "            self.review_df = self.review_df[self.critic_cols].copy()\n",
    "        return\n",
    "    \n",
    "    def write_df(self):\n",
    "        '''\n",
    "        If self.write_data == True, a pickled dataframe will be written\n",
    "        '''\n",
    "        if self.write_data == True:\n",
    "            self.review_df.to_pickle(f'data/{self.reviewer}_{self.movie_title}.pkl')\n",
    "    \n",
    "    def run_for_reviews(self):\n",
    "        '''\n",
    "        runs all class methods -> scrape_reviews, filter_df, and write_df \n",
    "        '''\n",
    "        try: \n",
    "            self.scrape_reviews()\n",
    "            self.filter_df()\n",
    "            self.write_df()\n",
    "            return self.review_df\n",
    "        \n",
    "        except: \n",
    "            print(f'Could not find reviews for *{self.movie_title}*... Please try to find another one' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac171a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_movie_names = [m.split('m/')[-1] for m in df.url.values]\n",
    "rt_movie_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5edb98a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "failed_critic = []\n",
    "\n",
    "scraper = ScrapeRtForDB(movie_title = None, \n",
    "                      scraping_limit= 200, \n",
    "                      reviewer = 'critic', \n",
    "                      write_data = True\n",
    "                    )\n",
    "\n",
    "for title in tqdm(rt_movie_names[:]):\n",
    "    try: \n",
    "        scraper.movie_title = title\n",
    "        scraper.run_for_reviews()\n",
    "    except: \n",
    "        failed_critic.append(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d80ef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_user = []\n",
    "\n",
    "scraper = ScrapeRtForDB(movie_title = None, \n",
    "                      scraping_limit= 10, \n",
    "                      reviewer = 'user', \n",
    "                      write_data = True\n",
    "                    )\n",
    "\n",
    "for title in tqdm(rt_movie_names[:]):\n",
    "    try: \n",
    "        scraper.movie_title = title\n",
    "        scraper.run_for_reviews()\n",
    "    except: \n",
    "        failed_user.append(title)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
